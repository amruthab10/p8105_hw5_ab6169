---
title: "p8105_hw5_ab6169"
author: "Amrutha Banda"
date: "2025-11-08"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(p8105.datasets)
library(tidyverse)
library(broom)
library(dplyr)
library(purrr)

```


# Problem 1 

```{r}
birthdays = sample(1:365, 50, replace = TRUE)

repeated_bday= length(unique(birthdays)) < 50

repeated_bday

unique(c(1,2,3,4,5,1,1,3))
```

In a function
```{r}
bday_sim= function(n_room) {
  birthdays= sample(1:365, n_room, replace = TRUE)

  repeated_bday= length(unique(birthdays)) < n_room

  repeated_bday
}

bday_sim(20)
```

```{r}
bday_sim_results= 
  expand_grid(
    bdays= 2:50, 
    iter= 1:10000
  ) |>  
  mutate(
    result= map_lgl(bdays, bday_sim)
  ) |> 
  group_by(
    bdays
  ) |>  
  summarize(
    prob_repeat= mean(result)
  )
```


Plot Showing the Probability as a function of group size 
```{r}
bday_sim_results |>
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() + 
  geom_line() +
  labs(
    x = "Group size (n)",
    y = "Probability of at least one shared birthday",
    title = "Birthday paradox via simulation (10,000 runs per n)"
  )
```
Comments: This plot shows us how probability of at least two people sharing a birthday increases as the group size grows. When there are fewer than 10 people in the room, the probability is close to zero. This means that having a shared birthday is very unlikely. Meanwhile, as the group size approaches around 20-25 people, the probability increases and reaches to about 50%. I also observed that the curve begins to flatten after about 35 people. 



# Problem 2 

```{r}
set.seed(1)
```

Simulating One Sample test
```{r}
sim_ttest = function(mu, n = 30, sigma = 5) {
  
  x  = rnorm(n, mean = mu, sd = sigma)
  tt = t.test(x, mu = 0)
  tibble(
    mu_hat  = mean(x),
    p_value = tt$p.value,
    reject  = tt$p.value < 0.05
  )
}

```

5000 simulations 
```{r}
sims_df = 
  expand_grid(
    mu_true = 0:6,
    iter = 1:5000
  ) |>
  mutate(
    res = map(mu_true, ~ sim_ttest(mu = .x))
  ) |>
  unnest(res)
```


Power of the test 
```{r}
power_df =
  sims_df |>
  group_by(mu_true) |>
  summarize(
    power = mean(reject),
    .groups = "drop"
  )
```

Power Plot 
```{r}
power_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power vs Effect Size (one-sample t-test; n=30, σ=5, α=0.05)",
    x = "True Mean (μ)",
    y = "Power (Pr reject H0)"
  )
```
Description: I observed that as the effect size (true mean) increases, the power of the test also increases. I see that when the true mean is is close to 0, the power is low. As the effect size gets larger, there is higher statistical power. This basically means that the probability of rejecting the null rises sharply. 


Average Estimate 
```{r}
estimate_df =
  sims_df |>
  group_by(mu_true) |>
  summarize(
    mean_mu_hat_all = mean(mu_hat),
    mean_mu_hat_reject = mean(mu_hat[reject]),
    .groups = "drop"
  )

estimate_df
```

Individual Plot of Average Estimate
```{r}
estimate_df |>
  ggplot(aes(x = mu_true, y = mean_mu_hat_all)) +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  labs(
    title = "Average Estimate of μ̂ vs True μ",
    x = "True Mean",
    y = "Average Estimate of μ̂"
  )
```


Individual Plot: Null Rejected
```{r}
estimate_df |>
  ggplot(aes(x = mu_true, y = mean_mu_hat_reject)) +
  geom_point(color = "red") +
  geom_line(color = "red") +
  labs(
    title = "Average μ̂ (only when p < 0.05)",
    x = "True Mean (μ)",
    y = "Average Estimate of μ̂"
  )
```


Comments: The sample average of μ̂ across tests for which the null is rejected is not approximately equal to the true value of μ. I observed that when we only include samples where the null hypothesis was rejected (p < 0.05), the estimated means tend to be larger than the true mean. This is happening because we’re only keeping samples show stronger effects than average.


# Problem 3 

Load the Homicide Dataset
```{r}
homicide_raw= read_csv("data/homicide-data.csv", na = c("NA", ".", "")) |> 
janitor::clean_names()
```
Description of Data: The contains information on homicide cases collected by The Washington Post from 50 major U.S. cities. It includes a total of `r nrow(homicide_raw)` observations and `r ncol(homicide_raw)` variables. Each row represents a single homicide incident and provides details such as the victim’s name, age, race, and sex, the date the case was reported, the city and state where it occurred, and the case disposition status. After cleaning the variable names using the `janitor::clean_names()` function, a new variable called `city_state` was created by combining the `city` and `state` columns 


```{r}

unsolved_levels= c("Closed without arrest", "Closed by arrest")

homicide_data= 
  homicide_raw |>
  mutate(
    city_state = paste(city, state, sep = ","),
    unsolved   = disposition %in% unsolved_levels) |>
  group_by(city_state) |>
  mutate(
    total= n(),
    unsolved_total = sum(unsolved)) |>
  ungroup() |>
  select(-city, -state, -disposition)

 
   
```

Baltimore, MD

```{r}

baltimore_df <- 
  homicide_data |> 
  filter(city_state %in% c("Baltimore, MD", "Baltimore,MD"))


baltimore_counts <- 
  baltimore_df |> 
  summarize(
    total = n(),
    unsolved = sum(unsolved, na.rm = TRUE)
  )
baltimore_counts

baltimore_test <- 
  prop.test(baltimore_counts$unsolved, baltimore_counts$total)
baltimore_test

baltimore_tidy <- 
  broom::tidy(baltimore_test) |> 
  select(estimate, conf.low, conf.high)

baltimore_tidy

```

Plot 
```{r}
city_counts <- homicide_data |>
  group_by(city_state) |>
  summarize(
    total    = n(),
    unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

city_results <- city_counts |>
  mutate(
    test_obj = map2(unsolved, total, ~ prop.test(.x, .y)),
    tidy_out = map(test_obj, broom::tidy)
  ) |>
  unnest(tidy_out) |>
  select(city_state, total, unsolved, estimate, conf.low, conf.high)
```

Plot: Estimates and CIs for each city 
```{r }
city_results |>
  mutate(city_state = forcats::fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Estimated proportion of unsolved homicides by city (95% CI)",
    x = NULL,
    y = "Proportion unsolved") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))
```


